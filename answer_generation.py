# -*- coding: utf-8 -*-
"""
answer_generation.py
====================
Stages 9 & 10 ‚Äì context building + answer generation.

‚Ä¢ build_context() ‚Äî —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å <<PAGE n>>.
‚Ä¢ generate_answer() ‚Äî –æ—Ç–¥–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON-–æ—Ç–≤–µ—Ç.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –º–æ–¥–µ–ª–∏:
  1. ü§ó transformers repo_id / –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ ‚Äî –æ–±—ã—á–Ω–∞—è FP16-–º–æ–¥–µ–ª—å;
  2. ü§ó repo_id, –æ–∫–∞–Ω—á–∏–≤–∞—é—â–∏–π—Å—è –Ω–∞ '-GGUF'            ‚Äì –∞–≤—Ç–æ-—Å–∫–∞—á–∫–∞ *.gguf;
  3. –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É *.gguf              ‚Äì —Å—Ä–∞–∑—É –≤ llama.cpp.

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (CPU-—Ä–µ–∂–∏–º):
    pip install transformers>=4.42 llama-cpp-python>=0.2.64 huggingface-hub
"""
from __future__ import annotations
from functools import lru_cache

import json
import re
import warnings
from pathlib import Path
from typing import List, Dict, Any, Tuple

from tqdm import tqdm

from prompts import build_final_prompt

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–∑–∞–≥—Ä—É–∂–∞–µ–º ¬´–ª–µ–Ω–∏–≤–æ¬ª, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å –±–µ–∑ GPU)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
except ImportError:  # pragma: no cover
    pipeline = AutoTokenizer = AutoModelForCausalLM = None  # type: ignore

try:
    from llama_cpp import Llama
except ImportError:  # pragma: no cover
    Llama = None  # type: ignore

try:
    from huggingface_hub import hf_hub_download, list_repo_files
except ImportError:  # pragma: no cover
    hf_hub_download = list_repo_files = None  # type: ignore

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  –ú–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DEFAULT_HF_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
DEFAULT_GGUF_REPO = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
_GGUF_PREFERRED = ("Q3_K_M.gguf", "Q4_K_M.gguf", "Q4_K.gguf", "Q5_0.gguf")  # —á—Ç–æ —Å–∫–∞—á–∏–≤–∞—Ç—å –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  9. CONTEXT BUILDING
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# def build_context(
#     ranked_chunks: List[Dict[str, Any]],
#     top_n: int = 10,
# ) -> Tuple[str, List[int]]:
#     ctx_lines, pages = [], []
#     for i, ch in enumerate(ranked_chunks[:top_n], 1):
#         ctx_lines.append(f"<<PAGE {i}>>\n{ch['text'].strip()}")
#         pages.append(i)
#     return "\n\n".join(ctx_lines), pages

# optional tokenizer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    import tiktoken
    _enc = tiktoken.get_encoding("cl100k_base")
except ImportError:
    _enc = None  # fallback ‚Üí –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å ¬´token = word¬ª

KEEP_CTX_TOKENS = 2048        # –º–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ–¥–∞–¥–∏–º –º–æ–¥–µ–ª–∏
# ---------------------------------------------------------------------------


def _encode(text: str) -> list[int]:
    if _enc:
        return _enc.encode(text)
    return text.split()        # –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞, –Ω–æ –ª—É—á—à–µ, —á–µ–º –Ω–∏—á–µ–≥–æ


def _decode(tokens: list[int] | list[str]) -> str:
    if _enc:
        return _enc.decode(tokens)          # type: ignore[arg-type]
    return " ".join(tokens)                 # fallback


def build_context(
    ranked_chunks: List[Dict[str, Any]],
    top_n: int = 10,
) -> Tuple[str, List[int]]:
    ctx_lines, pages = [], []
    for i, ch in enumerate(ranked_chunks[:top_n], 1):
        ctx_lines.append(f"<<PAGE {i}>>\n{ch['text'].strip()}")
        pages.append(i)

    ctx = "\n\n".join(ctx_lines)

    # -------- trim to KEEP_CTX_TOKENS ----------
    toks = _encode(ctx)
    if len(toks) > KEEP_CTX_TOKENS:
        toks = toks[-KEEP_CTX_TOKENS:]      # –æ—Å—Ç–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û —Ö–≤–æ—Å—Ç
        ctx = _decode(toks)

    return ctx, pages

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  helpers: –∑–∞–≥—Ä—É–∑–∫–∞ gguf, –≤—ã–±–æ—Ä backend
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def _download_best_gguf(repo_id: str, gguf_quantization: List[str] = _GGUF_PREFERRED) -> Path:
    if hf_hub_download is None or list_repo_files is None:
        raise RuntimeError("huggingface-hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    files = list_repo_files(repo_id)
    # –∏—â–µ–º ¬´–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π¬ª —Ñ–∞–π–ª
    for pat in gguf_quantization:
        match = next((f for f in files if f.endswith(pat)), None)
        if match:
            break
    else:
        # –±–µ—Ä—ë–º –ª—é–±–æ–π .gguf
        ggufs = [f for f in files if f.endswith(".gguf")]
        if not ggufs:
            raise FileNotFoundError("–≤ —Ä–µ–ø–æ –Ω–µ—Ç *.gguf")
        match = ggufs[0]

    local_path = hf_hub_download(repo_id, match, local_dir="~/.cache/gguf_models")
    return Path(local_path)


def _load_llm(model_ref: str | Path, max_ctx=4096, max_tok=512, gguf_quantization: List[str] = _GGUF_PREFERRED, verbose: bool = False):
    """
    –í—ã–±–∏—Ä–∞–µ—Ç backend:
      ‚Ä¢ .gguf ‚Üí llama.cpp
      ‚Ä¢ repo_id —Å ¬´-GGUF¬ª ‚Üí download+llama.cpp
      ‚Ä¢ –∏–Ω–∞—á–µ ‚Üí transformers.pipeline
    """
    model_ref = Path(model_ref) if not isinstance(model_ref, Path) else model_ref

    # ---------------- llama.cpp branch ----------------
    if str(model_ref).endswith(".gguf") or str(model_ref).endswith("-GGUF"):
        if Llama is None:
            raise RuntimeError("llama-cpp-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

        if str(model_ref).endswith("-GGUF"):
            model_ref = _download_best_gguf(str(model_ref), gguf_quantization)  # repo_id ‚Üí Path

        return Llama(
            model_path=str(model_ref),
            n_ctx=max_ctx,
            n_threads=4,
            n_gpu_layers=35,  # –∏–∑–º–µ–Ω–∏—Ç–µ, –µ—Å–ª–∏ —Å–æ–±—Ä–∞–ª–∏ CUDA/Metal
            verbose=verbose
        )

    # ---------------- transformers branch --------------
    if pipeline is None:
        raise RuntimeError("transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    tok = AutoTokenizer.from_pretrained(str(model_ref))
    mdl = AutoModelForCausalLM.from_pretrained(str(model_ref), device_map="auto")
    return pipeline(
        "text-generation",
        model=mdl,
        tokenizer=tok,
        max_new_tokens=max_tok,
        do_sample=False,
        verbose=False,
    )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 10. ANSWER GENERATION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _safe_json_extract(raw: str) -> Dict[str, Any]:
    try:
        start, end = raw.index("{"), raw.rindex("}") + 1
        return json.loads(raw[start:end])
    except Exception:
        return {"final_answer": "N/A"}


# def generate_answer(
#     query: str,
#     ranked_chunks: List[Dict[str, Any]],
#     model_ref: str | Path = DEFAULT_GGUF_REPO,
#     top_n_ctx: int = 10,
#     max_new_tokens: int = 512,
# ) -> Dict[str, Any]:
#     """Return structured dict as specified in prompts.JSON_SCHEMA."""
#     ctx, _ = build_context(ranked_chunks, top_n=top_n_ctx)
#     prompt = build_final_prompt(ctx, query)

#     llm = _load_llm(model_ref, max_ctx=4096, max_tok=max_new_tokens)

#     # llama.cpp ‚Üí –æ–±—ä–µ–∫—Ç Llama (callable), transformers ‚Üí pipeline
#     if isinstance(llm, Llama):
#         out = llm(prompt, max_tokens=max_new_tokens, temperature=0)["choices"][0]["text"]
#     else:
#         out = llm(prompt)[0]["generated_text"]

#     result = _safe_json_extract(out)
#     for k in ("step_by_step_analysis", "reasoning_summary", "citations", "final_answer"):
#         result.setdefault(k, "" if k != "citations" else [])
#     return result


@lru_cache(maxsize=1)
def _get_answer_llm(model_ref: str | Path, max_ctx=4096, max_tok=512, gguf_quantization: List[str] = _GGUF_PREFERRED, verbose=False):
    """–ï–¥–∏–Ω–æ—Ä–∞–∑–æ–≤–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç Llama –∏–ª–∏ transformers-pipeline."""
    return _load_llm(model_ref, max_ctx=max_ctx, max_tok=max_tok, gguf_quantization=gguf_quantization, verbose=verbose)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def generate_answer(
    query: str,
    ranked_chunks: List[Dict[str, Any]],
    model_ref: str | Path = DEFAULT_GGUF_REPO,
    top_n_ctx: int = 10,
    max_new_tokens: int = 512,
    gguf_quantization: List[str] = _GGUF_PREFERRED,
    verbose: int = False,
) -> Dict[str, Any]:
    ctx, _ = build_context(ranked_chunks, top_n=top_n_ctx)
    prompt = build_final_prompt(ctx, query)

    llm = _get_answer_llm(model_ref, max_ctx=4096, max_tok=max_new_tokens, gguf_quantization=gguf_quantization, verbose=verbose)

    if isinstance(llm, Llama):
        # llama.cpp –ø—É—Ç—å
        out = llm(prompt, max_tokens=max_new_tokens, temperature=0)["choices"][0]["text"]
    else:                                                # transformers.pipeline
        out = llm(prompt)[0]["generated_text"]

    result = _safe_json_extract(out)
    if verbose > 1:
        result['prompt'] = prompt  # –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    for k in ("step_by_step_analysis", "reasoning_summary", "citations", "final_answer"):
        result.setdefault(k, "" if k != "citations" else [])
    return result


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  CLI demo
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    import argparse
    import pickle

    parser = argparse.ArgumentParser(description="Generate structured answer (stages 9-10)")
    parser.add_argument("query", help="User question")
    parser.add_argument("ranked_pickle", type=Path, help="Pickle with ranked chunks (list[dict])")
    parser.add_argument(
        "--model",
        default=DEFAULT_GGUF_REPO,
        help="HF repo_id, gguf path, –∏–ª–∏ transformers-–º–æ–¥–µ–ª—å",
    )
    args = parser.parse_args()

    ranked = pickle.loads(args.ranked_pickle.read_bytes())
    ans = generate_answer(args.query, ranked, model_ref=args.model, gguf_quantization=_GGUF_PREFERRED)
    print(json.dumps(ans, indent=2, ensure_ascii=False))
