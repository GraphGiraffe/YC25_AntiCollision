# -*- coding: utf-8 -*-
"""
answer_generation.py
====================
Stages 9 & 10 â€“ context building + answer generation.

â€¢ build_context() â€” Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ <<PAGE n>>.
â€¢ generate_answer() â€” Ğ¾Ñ‚Ğ´Ğ°Ñ‘Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ JSON-Ğ¾Ñ‚Ğ²ĞµÑ‚.

ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:
  1. ğŸ¤— transformers repo_id / Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³ â€” Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ°Ñ FP16-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ;
  2. ğŸ¤— repo_id, Ğ¾ĞºĞ°Ğ½Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ½Ğ° '-GGUF'            â€“ Ğ°Ğ²Ñ‚Ğ¾-ÑĞºĞ°Ñ‡ĞºĞ° *.gguf;
  3. Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ *.gguf              â€“ ÑÑ€Ğ°Ğ·Ñƒ Ğ² llama.cpp.

Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ (CPU-Ñ€ĞµĞ¶Ğ¸Ğ¼):
    pip install transformers>=4.42 llama-cpp-python>=0.2.64 huggingface-hub
"""
from __future__ import annotations
from functools import lru_cache

import json
import re
import warnings
from pathlib import Path
from typing import List, Dict, Any, Tuple

from tqdm import tqdm

from prompts import build_final_prompt

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ (Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Â«Ğ»ĞµĞ½Ğ¸Ğ²Ğ¾Â», Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ±ĞµĞ· GPU)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
except ImportError:  # pragma: no cover
    pipeline = AutoTokenizer = AutoModelForCausalLM = None  # type: ignore

try:
    from llama_cpp import Llama
except ImportError:  # pragma: no cover
    Llama = None  # type: ignore

try:
    from huggingface_hub import hf_hub_download, list_repo_files
except ImportError:  # pragma: no cover
    hf_hub_download = list_repo_files = None  # type: ignore

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_HF_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
DEFAULT_GGUF_REPO = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
_GGUF_PREFERRED = ("Q3_K_M.gguf", "Q4_K_M.gguf", "Q4_K.gguf", "Q5_0.gguf")  # Ñ‡Ñ‚Ğ¾ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ğ²ÑĞµĞ³Ğ¾

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  9. CONTEXT BUILDING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# def build_context(
#     ranked_chunks: List[Dict[str, Any]],
#     top_n: int = 10,
# ) -> Tuple[str, List[int]]:
#     ctx_lines, pages = [], []
#     for i, ch in enumerate(ranked_chunks[:top_n], 1):
#         ctx_lines.append(f"<<PAGE {i}>>\n{ch['text'].strip()}")
#         pages.append(i)
#     return "\n\n".join(ctx_lines), pages

# optional tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import tiktoken
    _enc = tiktoken.get_encoding("cl100k_base")
except ImportError:
    _enc = None  # fallback â†’ Ğ±ÑƒĞ´ĞµĞ¼ ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Â«token = wordÂ»

KEEP_CTX_TOKENS = 2048        # Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ´Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
# ---------------------------------------------------------------------------


def _encode(text: str) -> list[int]:
    if _enc:
        return _enc.encode(text)
    return text.split()        # Ğ³Ñ€ÑƒĞ±Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°, Ğ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾


def _decode(tokens: list[int] | list[str]) -> str:
    if _enc:
        return _enc.decode(tokens)          # type: ignore[arg-type]
    return " ".join(tokens)                 # fallback


def build_context(
    ranked_chunks: List[Dict[str, Any]],
    top_n: int = 10,
) -> Tuple[str, List[int]]:
    ctx_lines, pages = [], []
    for i, ch in enumerate(ranked_chunks[:top_n], 1):
        ctx_lines.append(f"<<PAGE {i}>>\n{ch['text'].strip()}")
        pages.append(i)

    ctx = "\n\n".join(ctx_lines)

    # -------- trim to KEEP_CTX_TOKENS ----------
    toks = _encode(ctx)
    if len(toks) > KEEP_CTX_TOKENS:
        toks = toks[-KEEP_CTX_TOKENS:]      # Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¢ĞĞ›Ğ¬ĞšĞ Ñ…Ğ²Ğ¾ÑÑ‚
        ctx = _decode(toks)

    return ctx, pages

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  helpers: Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° gguf, Ğ²Ñ‹Ğ±Ğ¾Ñ€ backend
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _download_best_gguf(repo_id: str, gguf_quantization: List[str] = _GGUF_PREFERRED) -> Path:
    if hf_hub_download is None or list_repo_files is None:
        raise RuntimeError("huggingface-hub Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")

    files = list_repo_files(repo_id)
    # Ğ¸Ñ‰ĞµĞ¼ Â«Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹Â» Ñ„Ğ°Ğ¹Ğ»
    for pat in gguf_quantization:
        match = next((f for f in files if f.endswith(pat)), None)
        if match:
            break
    else:
        # Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ»ÑĞ±Ğ¾Ğ¹ .gguf
        ggufs = [f for f in files if f.endswith(".gguf")]
        if not ggufs:
            raise FileNotFoundError("Ğ² Ñ€ĞµĞ¿Ğ¾ Ğ½ĞµÑ‚ *.gguf")
        match = ggufs[0]

    local_path = hf_hub_download(repo_id, match, local_dir="~/.cache/gguf_models")
    return Path(local_path)


def _load_llm(model_ref: str | Path, max_ctx=4096, max_tok=512, gguf_quantization: List[str] = _GGUF_PREFERRED, verbose: bool = False):
    """
    Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ backend:
      â€¢ .gguf â†’ llama.cpp
      â€¢ repo_id Ñ Â«-GGUFÂ» â†’ download+llama.cpp
      â€¢ Ğ¸Ğ½Ğ°Ñ‡Ğµ â†’ transformers.pipeline
    """
    model_ref = Path(model_ref) if not isinstance(model_ref, Path) else model_ref

    # ---------------- llama.cpp branch ----------------
    if str(model_ref).endswith(".gguf") or str(model_ref).endswith("-GGUF"):
        if Llama is None:
            raise RuntimeError("llama-cpp-python Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")

        if str(model_ref).endswith("-GGUF"):
            model_ref = _download_best_gguf(str(model_ref), gguf_quantization)  # repo_id â†’ Path

        return Llama(
            model_path=str(model_ref),
            n_ctx=max_ctx,
            n_threads=4,
            n_gpu_layers=35,  # Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚Ğµ, ĞµÑĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ CUDA/Metal
            verbose=verbose
        )

    # ---------------- transformers branch --------------
    if pipeline is None:
        raise RuntimeError("transformers Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")

    tok = AutoTokenizer.from_pretrained(str(model_ref))
    mdl = AutoModelForCausalLM.from_pretrained(str(model_ref), device_map="auto")
    return pipeline(
        "text-generation",
        model=mdl,
        tokenizer=tok,
        max_new_tokens=max_tok,
        do_sample=False,
        verbose=False,
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 10. ANSWER GENERATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _safe_json_extract(raw: str) -> Dict[str, Any]:
    try:
        start, end = raw.index("{"), raw.rindex("}") + 1
        return json.loads(raw[start:end])
    except Exception:
        return {"final_answer": "N/A"}


# def generate_answer(
#     query: str,
#     ranked_chunks: List[Dict[str, Any]],
#     model_ref: str | Path = DEFAULT_GGUF_REPO,
#     top_n_ctx: int = 10,
#     max_new_tokens: int = 512,
# ) -> Dict[str, Any]:
#     """Return structured dict as specified in prompts.JSON_SCHEMA."""
#     ctx, _ = build_context(ranked_chunks, top_n=top_n_ctx)
#     prompt = build_final_prompt(ctx, query)

#     llm = _load_llm(model_ref, max_ctx=4096, max_tok=max_new_tokens)

#     # llama.cpp â†’ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Llama (callable), transformers â†’ pipeline
#     if isinstance(llm, Llama):
#         out = llm(prompt, max_tokens=max_new_tokens, temperature=0)["choices"][0]["text"]
#     else:
#         out = llm(prompt)[0]["generated_text"]

#     result = _safe_json_extract(out)
#     for k in ("step_by_step_analysis", "reasoning_summary", "citations", "final_answer"):
#         result.setdefault(k, "" if k != "citations" else [])
#     return result


@lru_cache(maxsize=1)
def _get_answer_llm(model_ref: str | Path, max_ctx=4096, max_tok=512, gguf_quantization: List[str] = _GGUF_PREFERRED, verbose=False):
    """Ğ•Ğ´Ğ¸Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Llama Ğ¸Ğ»Ğ¸ transformers-pipeline."""
    return _load_llm(model_ref, max_ctx=max_ctx, max_tok=max_tok, gguf_quantization=gguf_quantization, verbose=verbose)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def generate_answer(
    query: str,
    ranked_chunks: List[Dict[str, Any]],
    model_ref: str | Path = DEFAULT_GGUF_REPO,
    top_n_ctx: int = 10,
    max_new_tokens: int = 512,
    gguf_quantization: List[str] = _GGUF_PREFERRED,
    verbose: int = False,
) -> Dict[str, Any]:
    ctx, _ = build_context(ranked_chunks, top_n=top_n_ctx)
    prompt = build_final_prompt(ctx, query)

    llm = _get_answer_llm(model_ref, max_ctx=4096, max_tok=max_new_tokens, gguf_quantization=gguf_quantization, verbose=verbose)

    if isinstance(llm, Llama):
        # llama.cpp Ğ¿ÑƒÑ‚ÑŒ
        out = llm(prompt, max_tokens=max_new_tokens, temperature=0)["choices"][0]["text"]
    else:                                                # transformers.pipeline
        out = llm(prompt)[0]["generated_text"]

    result = _safe_json_extract(out)
    if verbose > 1:
        result['prompt'] = prompt  # Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸
    for k in ("step_by_step_analysis", "reasoning_summary", "citations", "final_answer"):
        result.setdefault(k, "" if k != "citations" else [])
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CLI demo
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import argparse
    import pickle

    parser = argparse.ArgumentParser(description="Generate structured answer (stages 9-10)")
    parser.add_argument("query", help="User question")
    parser.add_argument("ranked_pickle", type=Path, help="Pickle with ranked chunks (list[dict])")
    parser.add_argument(
        "--model",
        default=DEFAULT_GGUF_REPO,
        help="HF repo_id, gguf path, Ğ¸Ğ»Ğ¸ transformers-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ",
    )
    args = parser.parse_args()

    ranked = pickle.loads(args.ranked_pickle.read_bytes())
    ans = generate_answer(args.query, ranked, model_ref=args.model, gguf_quantization=_GGUF_PREFERRED)
    print(json.dumps(ans, indent=2, ensure_ascii=False))
