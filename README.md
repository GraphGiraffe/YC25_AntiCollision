## `README.md`

> **Solar-System RAG** ‚Äì an end-to-end, fully-offline Retrieval-Augmented-Generation
> workflow that fact-checks questions about the Sun, planets, moons, dwarf-planets,
> comets & belts.
>
> **Stack:** Python 3.11 ‚Ä¢ Conda + pip ‚Ä¢ Sentence-Transformers ‚Ä¢ FAISS ‚Ä¢ Cross-Encoder
> rerank ‚Ä¢ `llama.cpp`/Transformers for answer-LLM

---
### Pipeline overview

| #      | Stage                        | What happens                                                                                                                                                                                           | Why                                                                    | \*\*Tools used in **this repo**                                         |
| ------ | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| **0**  | **Env setup**                | Conda + pip `env.yml` installs: Py 3.11, `pandas`, `sentence-transformers`, `faiss-gpu/-cpu`, `llama-cpp-python`, `transformers`, ‚Ä¶                                                                    | Reproducible, GPU/CPU-agnostic sandbox                                 | Conda + pip (see `env.yml`)                                             |
| **1**  | Raw collection               | For each page in `SOLAR_SYSTEM_PAGES` call MediaWiki API ‚Üí save `*.raw.wiki` + meta JSON                                                                                                               | Freeze source text & revision stamp                                    | `requests` (REST) ‚Äî `raw_collection.py`                                 |
| **2**  | Parsing & cleaning           | `mwparserfromhell` ‚Üí plain text; strip templates, `[citation needed]`; convert units to SI (`pint`)                                                                                                    | Noise-free text = better embeddings                                    | `parsing_cleaning.py`                                                   |
| **3**  | Table serialisation *(opt.)* | `pandas.read_html` extracts infobox / physical-data tables ‚Üí one JSON line per row appended to text                                                                                                    | Extra structured facts for retrieval                                   | `table_serialisation.py`                                                |
| **4**  | Chunking                     | `tiktoken` ‚Üí 300-token windows, 50-token overlap; store `planet, section, char_start/end` in metadata                                                                                                  | Short, semantically tight chunks improve recall                        | `chunking.py`                                                           |
| **5**  | Vectorisation                | Encode chunks with **`BAAI/bge-base-en-v1.5`** (768 d); build one `faiss.IndexFlatIP` **per page**                                                                                                     | ‚ÄúOne doc = one index‚Äù isolates topics                                  | FAISS GPU/CPU, `sentence-transformers` ‚Äî `build_wiki_dataset.py`        |
| **6**  | Query routing                | Regex-match slugs via auto-aliases ‚Üí select subset of indices, fallback = all                                                                                                                          | 8-70√ó less search space, same recall                                   | `routing_utils.py`, `query_router.py`                                   |
| **7**  | First-pass retrieval         | Search each selected index for **top-k (30-50)** by cosine                                                                                                                                             | Brute-force Flat IP maximises recall                                   | `retrieval.py`                                                          |
| **8**  | Cross-encoder rerank         | Score each *(query, chunk)* pair with **`BAAI/bge-reranker-base`**; final = 0.3¬∑vector + 0.7¬∑CE                                                                                                        | Cheap precision boost, no generation latency                           | `sentence-transformers.CrossEncoder` ‚Äî `rerank_utils.py`                |
| **9**  | Context build                | Take top-10 chunks, prefix with `<<PAGE n>>`; trim to ‚â§2048 tokens                                                                                                                                     | Fits into 4 k ctx; page tags enable citations                          | `answer_generation.build_context()`                                     |
| **10** | Answer generation            | Prompt = system + JSON-schema + context + question ‚Üí **`Mistral-7B` GGUF** via `llama.cpp` ‚Üí strict JSON: `step_by_step_analysis`, `reasoning_summary`, `citations`, `final_answer` (‚ÄúN/A‚Äù if no fact) | CoT + structured output = minimal hallucinations, easy post-processing | `answer_generation.generate_answer()` (Transformers **or** `llama.cpp`) |


---

## 0 ¬∑ Repo layout

```
solar-rag/
 ‚îú‚îÄ build_wiki_dataset.py      # stages 1-5 ‚îÄ collect ‚Üí vectorise
 ‚îú‚îÄ query_router.py            # stage 6    ‚îÄ query ‚Üí index subset
 ‚îú‚îÄ retrieval.py               # stage 7    ‚îÄ FAISS Top-k
 ‚îú‚îÄ rerank_utils.py            # stage 8    ‚îÄ Cross-Encoder rerank
 ‚îú‚îÄ answer_generation.py       # stages 9-10 ‚îÄ context ‚Üí JSON answer
 ‚îú‚îÄ prompts.py                 # reusable system / schema prompts
 ‚îú‚îÄ test_* .py                 # quick demos for every stage-group
 ‚îú‚îÄ text_dicts.py              # list of Wikipedia pages + sample questions
 ‚îú‚îÄ env.yml                    # Conda + pip environment (CPU ‚úì / GPU ‚úì)
 ‚îî‚îÄ wiki/                      # will appear after stage-5
```

---

## 1 ¬∑ Set-up (one command)

```bash
conda env create -f env.yml      # installs conda + pip deps
conda activate ac                # (the env name is "ac")
```

### Optional ¬∑ GPU build of `llama.cpp`

```bash
# only if you DO have CUDA ‚â• 11.8
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 \
pip install --no-binary :all: llama-cpp-python==0.2.64
```

Check:

```bash
python - <<'PY'
import llama_cpp
print("llama.cpp ‚Üí", llama_cpp.__version__)
PY
```

---

## 2 ¬∑ Build the local corpus (stages 1-5)

```bash
python build_wiki_dataset.py          # ‚âà 4-5 min on laptop
# artefacts land in ./wiki/{raw|clean|chunks|faiss}
```

*Pages taken from* `text_dicts.SOLAR_SYSTEM_PAGES` ‚Äì 50 articles incl. planets,
moons, ‚ÄúMoons of Jupiter‚Äù, Sun, belts, comets ‚Ä¶

---

## 3 ¬∑ End-to-end test (stages 6-10)

```bash
python test_answer_generation.py       # answers 20 sample questions
```

Response is **strict JSON**:

```json
{
  "step_by_step_analysis": "According to <<PAGE 2>>, Mercury has...",
  "reasoning_summary": "No measurable atmosphere ‚Üí surface pressure ‚âà 0 Pa.",
  "citations": [2],
  "final_answer": "Effectively 0 Pa (near-vacuum)."
}
```

---

## 4 ¬∑ Performance knobs

| Layer          | Default                    | Swap to                          | Effect                      |
| -------------- | -------------------------- | -------------------------------- | --------------------------- |
| **Embeddings** | `bge-base-en-v1.5` (768 d) | `bge-large`                      | +1 pp nDCG, √ó1.4 RAM        |
| **Reranker**   | `bge-reranker-base`        | `bge-reranker-large`             | +2 pp nDCG, needs 3 GB VRAM |
| **Answer LLM** | `Mistral-7B Q4_K_M` GGUF   | GPU layers 35 or `phi-3-mini-4k` | 5 tok/s ‚Üí 25 tok/s          |
| **Prompt**     | 10 pages ‚Üí 3 600 tok       | `KEEP_CTX_TOKENS = 1_024`        | ‚Äì70 % prompt-eval time      |

Tune mixing weight in `rerank_utils.py`: `_ALPHA = 0.3`
(0 = CE-only, 1 = vector-only).

---

Happy hacking üöÄ
