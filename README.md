## `README.md`

> **Solar-System RAG** â€“ an end-to-end, fully-offline Retrieval-Augmented-Generation
> workflow that fact-checks questions about the Sun, planets, moons, dwarf-planets,
> comets & belts.
>
> **Stack:** Python 3.11 â€¢ Conda + pip â€¢ Sentence-Transformers â€¢ FAISS â€¢ Cross-Encoder
> rerank â€¢ `llama.cpp`/Transformers for answer-LLM

---
### RAG Scheme

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  build database  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    INGESTION  (stages 1-5, offline)                     â”‚
  â”‚                                                                         â”‚
  â”‚  Wikipedia pages  â†’  clean & chunk  â†’  BGE embeddings  â†’  FAISS index   â”‚
  â”‚                                                                         â”‚
  â”‚  (see: build_wiki_dataset.py â†’ wiki/{rawâ”‚cleanâ”‚chunksâ”‚faiss})           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  llm usage  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”€â”€â”€routeâ”€â”€â”€â–º  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Question â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â–ºâ”‚  Index selector  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  FAISS search â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚       â”‚  (routing_utils) â”‚         â”‚  (top-K = 30) â”‚  â”‚
                    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚                       selected *.faiss                â”‚
                    â”‚                                                       â”‚
                    â”‚                   pairs                               â”‚
                    â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
                    â”‚            â”‚ Cross-Encoder CE â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚            â”‚  (rerank_utils)  â”‚   vec-score Î± + CE 1-Î± 
                    â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        
                    â”‚                     â”‚ top-10 chunks                    
                    â”‚                     â–¼                                  
                    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     
                    â”‚         â”‚    build_context()     â”‚                     
                    â”‚         â”‚  <<PAGE n>> markers    â”‚                     
                    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     
                    â”‚                     â”‚ ctx (prompt string)              
                    â”‚                     â–¼                                  
                    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     
                    â”‚         â”‚    Prompt template     â”‚                     
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ JSON-schema + ctx + Q  â”‚                     
                              â”‚   (prompts.py)         â”‚                     
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     
                                          â”‚ final prompt                     
                                          â–¼                                  
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     
                              â”‚  Answer-LLM            â”‚                     
                              â”‚  llama.cpp  (GGUF)     â”‚                     
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     
                                          â”‚ JSON output                      
                                          â–¼                                  
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           
                                  â”‚   Answer     â”‚                           
                                  â”‚ (strict JSON)â”‚                           
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           

```
---

### Pipeline overview

| #      | Stage                        | What happens                                                                                                                                                                                           | Why                                                                    | \*\*Tools used in **this repo**                                         |
| ------ | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| **0**  | **Env setup**                | Conda + pip `env.yml` installs: Py 3.11, `pandas`, `sentence-transformers`, `faiss-gpu/-cpu`, `llama-cpp-python`, `transformers`, â€¦                                                                    | Reproducible, GPU/CPU-agnostic sandbox                                 | Conda + pip (see `env.yml`)                                             |
| **1**  | Raw collection               | For each page in `SOLAR_SYSTEM_PAGES` call MediaWiki API â†’ save `*.raw.wiki` + meta JSON                                                                                                               | Freeze source text & revision stamp                                    | `requests` (REST) â€” `raw_collection.py`                                 |
| **2**  | Parsing & cleaning           | `mwparserfromhell` â†’ plain text; strip templates, `[citation needed]`; convert units to SI (`pint`)                                                                                                    | Noise-free text = better embeddings                                    | `parsing_cleaning.py`                                                   |
| **3**  | Table serialisation *(opt.)* | `pandas.read_html` extracts infobox / physical-data tables â†’ one JSON line per row appended to text                                                                                                    | Extra structured facts for retrieval                                   | `table_serialisation.py`                                                |
| **4**  | Chunking                     | `tiktoken` â†’ 300-token windows, 50-token overlap; store `planet, section, char_start/end` in metadata                                                                                                  | Short, semantically tight chunks improve recall                        | `chunking.py`                                                           |
| **5**  | Vectorisation                | Encode chunks with **`BAAI/bge-base-en-v1.5`** (768 d); build one `faiss.IndexFlatIP` **per page**                                                                                                     | â€œOne doc = one indexâ€ isolates topics                                  | FAISS GPU/CPU, `sentence-transformers` â€” `build_wiki_dataset.py`        |
| **6**  | Query routing                | Regex-match slugs via auto-aliases â†’ select subset of indices, fallback = all                                                                                                                          | 8-70Ã— less search space, same recall                                   | `routing_utils.py`, `query_router.py`                                   |
| **7**  | First-pass retrieval         | Search each selected index for **top-k (30-50)** by cosine                                                                                                                                             | Brute-force Flat IP maximises recall                                   | `retrieval.py`                                                          |
| **8**  | Cross-encoder rerank         | Score each *(query, chunk)* pair with **`BAAI/bge-reranker-base`**; final = 0.3Â·vector + 0.7Â·CE                                                                                                        | Cheap precision boost, no generation latency                           | `sentence-transformers.CrossEncoder` â€” `rerank_utils.py`                |
| **9**  | Context build                | Take top-10 chunks, prefix with `<<PAGE n>>`; trim to â‰¤2048 tokens                                                                                                                                     | Fits into 4 k ctx; page tags enable citations                          | `answer_generation.build_context()`                                     |
| **10** | Answer generation            | Prompt = system + JSON-schema + context + question â†’ **`Mistral-7B` GGUF** via `llama.cpp` â†’ strict JSON: `step_by_step_analysis`, `reasoning_summary`, `citations`, `final_answer` (â€œN/Aâ€ if no fact) | CoT + structured output = minimal hallucinations, easy post-processing | `answer_generation.generate_answer()` (Transformers **or** `llama.cpp`) |

*(Stages 11-12 from the original checklistâ€”CLI wrappers & evaluationâ€”are not yet implemented in this fork, but hooks are ready in `test_*` scripts.)*

---

## 0 Â· Repo layout

```
solar-rag/
 â”œâ”€ build_wiki_dataset.py      # stages 1-5 â”€ collect â†’ vectorise
 â”œâ”€ query_router.py            # stage 6    â”€ query â†’ index subset
 â”œâ”€ retrieval.py               # stage 7    â”€ FAISS Top-k
 â”œâ”€ rerank_utils.py            # stage 8    â”€ Cross-Encoder rerank
 â”œâ”€ answer_generation.py       # stages 9-10 â”€ context â†’ JSON answer
 â”œâ”€ prompts.py                 # reusable system / schema prompts
 â”œâ”€ test_* .py                 # quick demos for every stage-group
 â”œâ”€ text_dicts.py              # list of Wikipedia pages + sample questions
 â”œâ”€ env.yml                    # Conda + pip environment (CPU âœ“ / GPU âœ“)
 â””â”€ wiki/                      # will appear after stage-5
```

---

## 1 Â· Set-up (one command)

```bash
conda env create -f env.yml      # installs conda + pip deps
conda activate ac                # (the env name is "ac")
```

### Optional Â· GPU build of `llama.cpp`

```bash
# only if you DO have CUDA â‰¥ 11.8
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 \
pip install --no-binary :all: llama-cpp-python==0.2.64
```

Check:

```bash
python - <<'PY'
import llama_cpp
print("llama.cpp â†’", llama_cpp.__version__)
PY
```

---

## 2 Â· Build the local corpus (stages 1-5)

```bash
python build_wiki_dataset.py          # â‰ˆ 4-5 min on laptop
# artefacts land in ./wiki/{rawâ”‚cleanâ”‚chunksâ”‚faiss}
```

*Pages taken from* `text_dicts.SOLAR_SYSTEM_PAGES` â€“ 50 articles incl. planets,
moons, â€œMoons of Jupiterâ€, Sun, belts, comets â€¦

---

## 3 Â· End-to-end test (stages 6-10)

```bash
python test_answer_generation.py       # answers 20 sample questions
```

Response is **strict JSON**:

```json
{
  "step_by_step_analysis": "According to <<PAGE 2>>, Mercury has...",
  "reasoning_summary": "No measurable atmosphere â†’ surface pressure â‰ˆ 0 Pa.",
  "citations": [2],
  "final_answer": "Effectively 0 Pa (near-vacuum)."
}
```

---

## 4 Â· Performance knobs

â”‚ Layer          â”‚ Default                    â”‚ Swap to                          â”‚ Effect                      â”‚
â”‚ -------------- â”‚ -------------------------- â”‚ -------------------------------- â”‚ --------------------------- â”‚
â”‚ **Embeddings** â”‚ `bge-base-en-v1.5` (768 d) â”‚ `bge-large`                      â”‚ +1 pp nDCG, Ã—1.4 RAM        â”‚
â”‚ **Reranker**   â”‚ `bge-reranker-base`        â”‚ `bge-reranker-large`             â”‚ +2 pp nDCG, needs 3 GB VRAM â”‚
â”‚ **Answer LLM** â”‚ `Mistral-7B Q4_K_M` GGUF   â”‚ GPU layers 35 or `phi-3-mini-4k` â”‚ 5 tok/s â†’ 25 tok/s          â”‚
â”‚ **Prompt**     â”‚ 10 pages â†’ 3 600 tok       â”‚ `KEEP_CTX_TOKENS = 1_024`        â”‚ â€“70 % prompt-eval time      â”‚

Tune mixing weight in `rerank_utils.py`: `_ALPHA = 0.3`
(0 = CE-only, 1 = vector-only).

---

Happy hacking ğŸš€
